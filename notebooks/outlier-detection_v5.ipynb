{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/network-attack-dataset-kitsune/SSL Renegotiation/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup environment\n!pip install pyspark\n# !pip install sparkmagic\n","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\n\u001b[K     |████████████████████████████████| 212.3 MB 13 kB/s s eta 0:00:01    |████████████                    | 80.0 MB 32.6 MB/s eta 0:00:05     |█████████████████████▌          | 142.9 MB 22.3 MB/s eta 0:00:04\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |████████████████████████████████| 198 kB 50.7 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=793d4d0720724ea14c72c5862da8a81707b8be3049a871a110c059e03a0fa4d9\n  Stored in directory: /root/.cache/pip/wheels/43/47/42/bc413c760cf9d3f7b46ab7cd6590e8c47ebfd19a7386cd4a57\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import monotonically_increasing_id, shiftRight, when, expr, lit, broadcast, sum\nfrom pyspark.sql.window import Window","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName(\"outlier-detection\").getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Dataset\nssl_reneg_dataset = \"../input/network-attack-dataset-kitsune/SSL Renegotiation/SSL_Renegotiation_dataset.csv\"\narp_spoof_dataset = \"../input/network-attack-dataset-kitsune/SSL Renegotiation/SSL_Renegotiation_dataset.csv\"\nsyn_dos_dataset = \"../input/network-attack-dataset-kitsune/SSL Renegotiation/SSL_Renegotiation_dataset.csv\"\n\n\ndf_ssl = spark.read.load(\n    ssl_reneg_dataset,\n    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\").withColumn(\"category\", lit(0))\ndf_arp = spark.read.load(\n    arp_spoof_dataset,\n    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\").withColumn(\"category\", lit(1))\ndf_syn = spark.read.load(\n    syn_dos_dataset,\n    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\").withColumn(\"category\", lit(2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature distrivution and analysis\n#LIMITS\ndf_ssl = df_ssl.limit(10000)\n# df_ssl.printSchema()\ndf_ssl.select(\"_c0\",\"_c1\", \"_c3\").describe().show()\ndf_arp = df_arp.limit(10000)\n# df_arp.printSchema()\ndf_arp.select(\"_c0\",\"_c1\", \"_c3\").describe().show()\ndf_syn = df_syn.limit(10000)\n# df_syn.printSchema()\ndf_syn.select(\"_c0\",\"_c1\", \"_c3\").describe().show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Union\ndf = df_ssl.union(df_arp)\ndf = df.union(df_syn)\ndf.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GENERATES ID**","metadata":{}},{"cell_type":"code","source":"# Generate monotonous id\ndf_data = df.withColumn(\"row_id\", monotonically_increasing_id())\n\n# analyze spark internal partitions\ndf_partition = df_data.withColumn(\"partition_id\", shiftRight('row_id', 33)) \\\n    .withColumn(\"row_offset\", df_data['row_id'].bitwiseAND(2147483647))\npartitions_size = df_partition.groupBy(\"partition_id\").count().withColumnRenamed(\"count\", \"partition_size\")\nwindowSpec = Window.orderBy(\"partition_id\").rowsBetween(Window.unboundedPreceding, -1) # warnings for window without a partition\n\n# Take care of the null partition_offsets\npartitions_offset = partitions_size.withColumn(\"partition_offset\", when(expr(\"partition_id = 0\"), lit(0))\n                                               .otherwise(sum(\"partition_size\").over(windowSpec)))\ndf_data_id = df_partition.join(broadcast(partitions_offset), \"partition_id\").withColumn(\"id\",partitions_offset.partition_offset + df_partition.row_offset + 1).drop(\"partition_id\", \"row_id\", \"row_offset\", \"partition_size\", \"partition_offset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#join with labels and filter only malign records","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**K-MEANS CLUSTERING**","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.pipeline import Pipeline\nfrom pyspark.sql import dataframe\nfrom pyspark.sql.functions import col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vecAssembler = VectorAssembler(\n    inputCols=[x for x in df_data_id.columns if x != \"label\"],\n    outputCol=\"features\"\n)\n\ndf_kmeans = vecAssembler.transform(df_data_id).select('id', 'features')\ndf_kmeans.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pyspark.ml.clustering import KMeans\n\nk_means = KMeans().setK(3).setSeed(1).setPredictionCol(\"cluster\").setFeaturesCol(\"features\")\nmodel = k_means.fit(df_kmeans)\ncenters = model.clusterCenters()\n# print(\"Centers: \",centers)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_df = model.transform(df_kmeans)\n# tf_df.select(\"id\", \"cluster\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_df = tf_df.join(df_data_id, 'id')\ntf_df.select(\"id\", \"_c0\", \"_c114\", \"cluster\").show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COnvet to pandas\npddf_pred = tf_df.toPandas().set_index('id')\npddf_pred.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nthreedee = plt.figure(figsize=(12,10)).gca(projection='3d')\nthreedee.scatter(pddf_pred._c0, pddf_pred._c1, pddf_pred._c114, c=pddf_pred.cluster)\nthreedee.set_xlabel('x')\nthreedee.set_ylabel('y')\nthreedee.set_zlabel('z')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}